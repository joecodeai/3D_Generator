{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PNG\n",
    "\n",
    "# from PIL import Image\n",
    "# import os\n",
    "\n",
    "# input_dir = r'D:\\Jupyter\\3D Construction\\Dataset\\redwood-3dscan\\data\\rgbd_extract\\00037\\rgb'\n",
    "# output_dir = r'D:\\Jupyter\\3D Construction\\Dataset\\redwood-3dscan\\data\\rgbd_extract\\00037\\rgb_png'\n",
    "\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# for filename in os.listdir(input_dir):\n",
    "#     if filename.lower().endswith('.jpg'):\n",
    "#         img = Image.open(os.path.join(input_dir, filename))\n",
    "#         # Convert to PNG\n",
    "#         png_filename = os.path.splitext(filename)[0] + '.png'\n",
    "#         img.save(os.path.join(output_dir, png_filename), 'PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Print the device to ensure it's set correctly\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the UNet model architecture\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.down1 = self.conv_block(1, 64)\n",
    "        self.down2 = self.conv_block(64, 128)\n",
    "        self.down3 = self.conv_block(128, 256)\n",
    "        self.down4 = self.conv_block(256, 512)\n",
    "        self.down5 = self.conv_block(512, 1024)\n",
    "        \n",
    "        self.up1 = self.upconv_block(1024, 512)\n",
    "        self.up2 = self.upconv_block(1024, 256)\n",
    "        self.up3 = self.upconv_block(512, 128)\n",
    "        self.up4 = self.upconv_block(256, 64)\n",
    "        self.final_depth = nn.Conv2d(128, 1, kernel_size=1)  # Depth map output\n",
    "        self.final_rgb = nn.Conv2d(128, 3, kernel_size=1)    # RGB image output\n",
    "    \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        \n",
    "        up1 = self.up1(d5)\n",
    "        up1 = torch.cat((up1, d4), dim=1)\n",
    "        up2 = self.up2(up1)\n",
    "        up2 = torch.cat((up2, d3), dim=1)\n",
    "        up3 = self.up3(up2)\n",
    "        up3 = torch.cat((up3, d2), dim=1)\n",
    "        up4 = self.up4(up3)\n",
    "        up4 = torch.cat((up4, d1), dim=1)\n",
    "        \n",
    "        depth_output = self.final_depth(up4)\n",
    "        rgb_output = self.final_rgb(up4)\n",
    "        \n",
    "        return depth_output, rgb_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class ChairDataset(Dataset):\n",
    "    def __init__(self, edge_dir, depth_dir, rgb_dir, transform=None):\n",
    "        self.edge_dir = edge_dir\n",
    "        self.depth_dir = depth_dir\n",
    "        self.rgb_dir = rgb_dir\n",
    "        self.transform = transform\n",
    "        self.edge_files = os.listdir(edge_dir)\n",
    "        self.valid_files = self._filter_valid_files()\n",
    "    \n",
    "    def _filter_valid_files(self):\n",
    "        valid_files = []\n",
    "        for filename in self.edge_files:\n",
    "            edge_path = os.path.join(self.edge_dir, filename)\n",
    "            depth_path = os.path.join(self.depth_dir, filename)\n",
    "            rgb_path = os.path.join(self.rgb_dir, filename)\n",
    "            if os.path.exists(edge_path) and os.path.exists(depth_path) and os.path.exists(rgb_path):\n",
    "                valid_files.append(filename)\n",
    "        return valid_files\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.valid_files[idx]\n",
    "        edge_path = os.path.join(self.edge_dir, filename)\n",
    "        depth_path = os.path.join(self.depth_dir, filename)\n",
    "        rgb_path = os.path.join(self.rgb_dir, filename)\n",
    "        \n",
    "        edge = Image.open(edge_path).convert('L')\n",
    "        depth = Image.open(depth_path).convert('L')\n",
    "        rgb = Image.open(rgb_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            edge = self.transform(edge)\n",
    "            depth = self.transform(depth)\n",
    "            rgb = self.transform(rgb)\n",
    "        \n",
    "        return edge, depth, rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_model(model, dataloader, criterion_depth, criterion_rgb, optimizer, num_epochs=10, device='cuda'):\n",
    "    model.to(device)  # Move the model to the specified device\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss_depth = 0.0\n",
    "        running_loss_rgb = 0.0\n",
    "        \n",
    "        total_samples = 0\n",
    "        correct_depth_predictions = 0  # Placeholder for depth accuracy\n",
    "        correct_rgb_predictions = 0    # Placeholder for RGB accuracy\n",
    "        \n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, depth_targets, rgb_targets = data\n",
    "            \n",
    "            # Move inputs and targets to the same device as the model\n",
    "            inputs = inputs.to(device)\n",
    "            depth_targets = depth_targets.to(device)\n",
    "            rgb_targets = rgb_targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            depth_outputs, rgb_outputs = model(inputs)\n",
    "            \n",
    "            # Print shapes for debugging\n",
    "            # print(f\"Depth Output Shape: {depth_outputs.shape}\")\n",
    "            # print(f\"Depth Target Shape: {depth_targets.shape}\")\n",
    "            # print(f\"RGB Output Shape: {rgb_outputs.shape}\")\n",
    "            # print(f\"RGB Target Shape: {rgb_targets.shape}\")\n",
    "            \n",
    "            # Resize targets to match the output sizes\n",
    "            depth_targets_resized = F.interpolate(depth_targets, size=depth_outputs.shape[2:], mode='bilinear', align_corners=False)\n",
    "            rgb_targets_resized = F.interpolate(rgb_targets, size=rgb_outputs.shape[2:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "            # Compute losses\n",
    "            loss_depth = criterion_depth(depth_outputs, depth_targets_resized)\n",
    "            loss_rgb = criterion_rgb(rgb_outputs, rgb_targets_resized)\n",
    "            loss = loss_depth + loss_rgb\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update loss statistics\n",
    "            running_loss_depth += loss_depth.item()\n",
    "            running_loss_rgb += loss_rgb.item()\n",
    "\n",
    "            # Accuracy (or another metric) calculation\n",
    "            # Placeholder: For depth and RGB, you may need to define appropriate metrics\n",
    "            # Here we're just accumulating the losses as an example\n",
    "            total_samples += inputs.size(0)\n",
    "            # Example accuracy calculation (needs to be customized based on your problem)\n",
    "            # For example, if your outputs are probabilities or logits:\n",
    "            # _, predicted_depth = torch.max(depth_outputs, 1)\n",
    "            # correct_depth_predictions += (predicted_depth == depth_targets_resized).sum().item()\n",
    "            # _, predicted_rgb = torch.max(rgb_outputs, 1)\n",
    "            # correct_rgb_predictions += (predicted_rgb == rgb_targets_resized).sum().item()\n",
    "\n",
    "        # Print average loss for the epoch\n",
    "        avg_loss_depth = running_loss_depth / len(dataloader)\n",
    "        avg_loss_rgb = running_loss_rgb / len(dataloader)\n",
    "        # Example accuracy (needs to be customized based on actual metrics)\n",
    "        # accuracy_depth = (correct_depth_predictions / total_samples) * 100\n",
    "        # accuracy_rgb = (correct_rgb_predictions / total_samples) * 100\n",
    "        print(f\"[Epoch {epoch + 1}] Avg Loss Depth: {avg_loss_depth:.3f}, Avg Loss RGB: {avg_loss_rgb:.3f}\")\n",
    "        # print(f\"Avg Accuracy Depth: {accuracy_depth:.2f}%\")\n",
    "        # print(f\"Avg Accuracy RGB: {accuracy_rgb:.2f}%\")\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "edge_dir = r'D:\\Jupyter\\3D Construction\\Dataset\\redwood-3dscan\\data\\rgbd_extract\\00037\\edge'\n",
    "depth_dir = r'D:\\Jupyter\\3D Construction\\Dataset\\redwood-3dscan\\data\\rgbd_extract\\00037\\depth'\n",
    "rgb_dir = r'D:\\Jupyter\\3D Construction\\Dataset\\redwood-3dscan\\data\\rgbd_extract\\00037\\rgb_png'\n",
    "\n",
    "dataset = ChairDataset(edge_dir, depth_dir, rgb_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "model = UNet().cuda()\n",
    "criterion_depth = nn.MSELoss()\n",
    "criterion_rgb = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Avg Loss Depth: 0.045, Avg Loss RGB: 0.037\n",
      "[Epoch 2] Avg Loss Depth: 0.025, Avg Loss RGB: 0.013\n",
      "[Epoch 3] Avg Loss Depth: 0.018, Avg Loss RGB: 0.008\n",
      "[Epoch 4] Avg Loss Depth: 0.015, Avg Loss RGB: 0.006\n",
      "[Epoch 5] Avg Loss Depth: 0.012, Avg Loss RGB: 0.005\n",
      "[Epoch 6] Avg Loss Depth: 0.010, Avg Loss RGB: 0.004\n",
      "[Epoch 7] Avg Loss Depth: 0.009, Avg Loss RGB: 0.004\n",
      "[Epoch 8] Avg Loss Depth: 0.008, Avg Loss RGB: 0.003\n",
      "[Epoch 9] Avg Loss Depth: 0.007, Avg Loss RGB: 0.003\n",
      "[Epoch 10] Avg Loss Depth: 0.006, Avg Loss RGB: 0.003\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, dataloader, criterion_depth, criterion_rgb, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary\n",
    "model = UNet()\n",
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (down1): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (down2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (down3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (down4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (down5): Sequential(\n",
       "    (0): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (up1): Sequential(\n",
       "    (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (up2): Sequential(\n",
       "    (0): ConvTranspose2d(1024, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (up3): Sequential(\n",
       "    (0): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (up4): Sequential(\n",
       "    (0): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (final_depth): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (final_rgb): Conv2d(128, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "load_model = UNet()\n",
    "\n",
    "# Load the state dictionary\n",
    "load_model.load_state_dict(torch.load('model_weights.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "load_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated images saved to D:\\Jupyter\\3D Construction\\Dataset\\redwood-3dscan\\data\\rgbd_extract\\00037\\output\\1_depth.png and D:\\Jupyter\\3D Construction\\Dataset\\redwood-3dscan\\data\\rgbd_extract\\00037\\output\\1_rgb.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the UNet model architecture with Sigmoid activation for RGB output\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.down1 = self.conv_block(1, 64)\n",
    "        self.down2 = self.conv_block(64, 128)\n",
    "        self.down3 = self.conv_block(128, 256)\n",
    "        self.down4 = self.conv_block(256, 512)\n",
    "        self.down5 = self.conv_block(512, 1024)\n",
    "        \n",
    "        self.up1 = self.upconv_block(1024, 512)\n",
    "        self.up2 = self.upconv_block(1024, 256)\n",
    "        self.up3 = self.upconv_block(512, 128)\n",
    "        self.up4 = self.upconv_block(256, 64)\n",
    "        self.final_depth = nn.Conv2d(128, 1, kernel_size=1)  # Depth map output\n",
    "        self.final_rgb = nn.Conv2d(128, 3, kernel_size=1)    # RGB image output\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid activation for RGB output\n",
    "    \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        \n",
    "        up1 = self.up1(d5)\n",
    "        up1 = torch.cat((up1, d4), dim=1)\n",
    "        up2 = self.up2(up1)\n",
    "        up2 = torch.cat((up2, d3), dim=1)\n",
    "        up3 = self.up3(up2)\n",
    "        up3 = torch.cat((up3, d2), dim=1)\n",
    "        up4 = self.up4(up3)\n",
    "        up4 = torch.cat((up4, d1), dim=1)\n",
    "        \n",
    "        depth_output = self.final_depth(up4)\n",
    "        rgb_output = self.sigmoid(self.final_rgb(up4))  # Apply Sigmoid activation\n",
    "        \n",
    "        return depth_output, rgb_output\n",
    "\n",
    "# Function to load an image, perform transformations, and get model predictions\n",
    "def generate_images(model, edge_image_path, depth_output_path, rgb_output_path, transform=None, device='cuda'):\n",
    "    model.to(device)  # Move the model to the specified device\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Load and transform the edge image\n",
    "    edge_image = Image.open(edge_image_path).convert('L')\n",
    "    if transform:\n",
    "        edge_image = transform(edge_image)\n",
    "    edge_image = edge_image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        depth_output, rgb_output = model(edge_image)\n",
    "    \n",
    "    # Convert outputs to images\n",
    "    depth_output = depth_output.squeeze(0).cpu().numpy()  # Remove batch dimension and move to CPU\n",
    "    rgb_output = rgb_output.squeeze(0).permute(1, 2, 0).cpu().numpy()  # Convert to HWC format and move to CPU\n",
    "    \n",
    "    # Save depth output\n",
    "    plt.imsave(depth_output_path, depth_output[0], cmap='gray')\n",
    "    \n",
    "    # Normalize and save RGB output\n",
    "    rgb_output = (rgb_output - rgb_output.min()) / (rgb_output.max() - rgb_output.min())  # Normalize to [0, 1]\n",
    "    plt.imsave(rgb_output_path, rgb_output)\n",
    "    \n",
    "    print(f\"Generated images saved to {depth_output_path} and {rgb_output_path}\")\n",
    "\n",
    "# Example usage\n",
    "edge_image_path = r'D:\\Jupyter\\3D Construction\\Dataset\\redwood-3dscan\\data\\rgbd_extract\\00037\\edge\\1.png'\n",
    "depth_output_path = r'D:\\Jupyter\\3D Construction\\Dataset\\redwood-3dscan\\data\\rgbd_extract\\00037\\output\\1_depth.png'\n",
    "rgb_output_path = r'D:\\Jupyter\\3D Construction\\Dataset\\redwood-3dscan\\data\\rgbd_extract\\00037\\output\\1_rgb.png'\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the trained model\n",
    "load_model = UNet()\n",
    "load_model.load_state_dict(torch.load('model_weights.pth'))\n",
    "\n",
    "# Generate images\n",
    "generate_images(load_model, edge_image_path, depth_output_path, rgb_output_path, transform=transform)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
